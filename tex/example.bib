@misc{.,
  title = {Bauhaus-{{Universität Weimar}}: {{3D-RealityCapture-ScanLab}}},
  url = {https://www.uni-weimar.de/de/medien/professuren/medieninformatik/computer-vision/forschung/3d-realitycapture-scanlab/},
  urldate = {2024-09-11}
}

@article{Abdal.2023,
  title = {{{3DAvatarGAN}}: {{Bridging Domains}} for {{Personalized Editable Avatars}}},
  author = {Abdal, Rameen and Lee, Hsin-Ying and Zhu, Peihao and Chai, Menglei and Siarohin, Aliaksandr and Wonka, Peter and Tulyakov, Sergey},
  date = {2023},
  doi = {10.1109/CVPR52729.2023.00442},
  abstract = {Modern 3D-GANs synthesize geometry and texture by training on large-scale datasets with a consistent structure. Training such models on stylized, artistic data, with often unknown, highly variable geometry, and camera information has not yet been shown possible. Can we train a 3D GAN on such artistic data, while maintaining multi-view consistency and texture quality? To this end, we propose an adaptation framework, where the source domain is a pre-trained 3D-GAN, while the target domain is a 2D-GAN trained on artistic datasets. We then distill the knowledge from a 2D generator to the source 3D generator. To do that, we first propose an optimization-based method to align the distributions of camera parameters across domains. Second, we propose regularizations necessary to learn high-quality texture, while avoiding degenerate geometric solutions, such as flat shapes. Third, we show a deformation-based technique for modeling exaggerated geometry of artistic domains, enabling – as a byproduct – personalized geometric editing. Finally, we propose a novel inversion method for 3D-GANs linking the latent spaces of the source and the target domains. Our contributions – for the first time – allow for the generation, editing, and animation of personalized artistic 3D avatars on artistic datasets.}
}

@misc{AUTOMATIC1111.2024,
  title = {{{AUTOMATIC1111}}/{{Stable-Diffusion-Webui}}: {{Stable Diffusion Web UI}}},
  author = {{AUTOMATIC1111}},
  editor = {{AUTOMATIC1111}},
  date = {2024},
  abstract = {Stable Diffusion web UI. Contribute to AUTOMATIC1111/stable-diffusion-webui development by creating an account on GitHub.}
}

@misc{Baker.2023,
  title = {{{DSSIM}}: A Structural Similarity Index for Floating-Point Data},
  shorttitle = {{{DSSIM}}},
  author = {Baker, Allison H. and Pinard, Alexander and Hammerling, Dorit M.},
  date = {2023-03},
  eprint = {2202.02616},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2202.02616},
  abstract = {Data visualization is a critical component in terms of interacting with floating-point output data from large model simulation codes. Indeed, postprocessing analysis workflows on simulation data often generate a large number of images from the raw data, many of which are then compared to each other or to specified reference images. In this image-comparison scenario, image quality assessment (IQA) measures are quite useful, and the Structural Similarity Index (SSIM) continues to be a popular choice. However, generating large numbers of images can be costly, and plot-specific (but data independent) choices can affect the SSIM value. A natural question is whether we can apply the SSIM directly to the floating-point simulation data and obtain an indication of whether differences in the data are likely to impact a visual assessment, effectively bypassing the creation of a specific set of images from the data. To this end, we propose an alternative to the popular SSIM that can be applied directly to the floating point data, which we refer to as the Data SSIM (DSSIM). While we demonstrate the usefulness of the DSSIM in the context of evaluating differences due to lossy compression on large volumes of simulation data from a popular climate model, the DSSIM may prove useful for many other applications involving simulation or image data.},
  organization = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Computation}
}

@misc{Ban.2024,
  title = {Understanding the {{Impact}} of {{Negative Prompts}}: {{When}} and {{How Do They Take Effect}}?},
  shorttitle = {Understanding the {{Impact}} of {{Negative Prompts}}},
  author = {Ban, Yuanhao and Wang, Ruochen and Zhou, Tianyi and Cheng, Minhao and Gong, Boqing and Hsieh, Cho-Jui},
  date = {2024-06},
  eprint = {2406.02965},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.02965},
  abstract = {The concept of negative prompts, emerging from conditional generation models like Stable Diffusion, allows users to specify what to exclude from the generated images.\%, demonstrating significant practical efficacy. Despite the widespread use of negative prompts, their intrinsic mechanisms remain largely unexplored. This paper presents the first comprehensive study to uncover how and when negative prompts take effect. Our extensive empirical analysis identifies two primary behaviors of negative prompts. Delayed Effect: The impact of negative prompts is observed after positive prompts render corresponding content. Deletion Through Neutralization: Negative prompts delete concepts from the generated image through a mutual cancellation effect in latent space with positive prompts. These insights reveal significant potential real-world applications; for example, we demonstrate that negative prompts can facilitate object inpainting with minimal alterations to the background via a simple adaptive algorithm. We believe our findings will offer valuable insights for the community in capitalizing on the potential of negative prompts.},
  organization = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{Bassier.2020,
  title = {Point {{Cloud}} vs. {{Mesh Features}} for {{Building Interior Classification}}},
  author = {Bassier, Maarten and Vergauwen, Maarten and Poux, Florent},
  date = {2020},
  journaltitle = {Remote Sensing},
  volume = {12},
  number = {14},
  pages = {2224},
  issn = {2072-4292},
  doi = {10.3390/rs12142224},
  abstract = {PDF | Interpreting 3D point cloud data of the interior and exterior of buildings is essential for automated navigation, interaction and 3D... | Find, read and cite all the research you need on ResearchGate}
}

@misc{benrugg.2024,
  title = {Benrugg/{{AI-render}}: {{Stable Diffusion}} in {{Blender}}},
  author = {{benrugg}},
  date = {2024},
  abstract = {Stable Diffusion in Blender. Contribute to benrugg/AI-Render development by creating an account on GitHub.}
}

@misc{Brooks.2023,
  title = {{{InstructPix2Pix}}: {{Learning}} to {{Follow Image Editing Instructions}}},
  shorttitle = {{{InstructPix2Pix}}},
  author = {Brooks, Tim and Holynski, Aleksander and Efros, Alexei A.},
  date = {2023-01},
  eprint = {2211.09800},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2211.09800},
  abstract = {We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models – a language model (GPT-3) and a text-to-image model (Stable Diffusion) – to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.},
  organization = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning}
}

@misc{Brooks.2024,
  title = {Timothybrooks/{{Instruct-Pix2pix}}},
  author = {Brooks, Tim},
  date = {2024-09}
}

@misc{Brown.2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07},
  eprint = {2005.14165},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2005.14165},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  organization = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@article{Canales.2024,
  title = {The {{Impact}} of {{Avatar Stylization}} on {{Trust}}},
  author = {Canales, Ryan and Roble, Doug and Neff, Michael},
  date = {2024},
  doi = {10.1109/VR58804.2024.00063},
  abstract = {VGTC Special Issue Paper for TVCG}
}

@inproceedings{Cao.2020,
  title = {{{PSNet}}: {{A Style Transfer Network}} for {{Point Cloud Stylization}} on {{Geometry}} and {{Color}}},
  booktitle = {2020 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Cao, Xu and Wang, Weimin and Nagao, Katashi and Nakamura, Ryosuke},
  date = {2020},
  pages = {3326--3334},
  publisher = {IEEE},
  location = {Piscataway, NJ},
  doi = {10.1109/WACV45572.2020.9093513},
  isbn = {978-1-72816-553-0}
}

@misc{Chen.2023,
  title = {Advances in {{3D Neural Stylization}}: {{A Survey}}},
  author = {Chen, Yingshu and Shao, Guocheng and Shum, Ka Chun and Hua, Binh-Son and Yeung, Sai-Kit},
  date = {2023},
  abstract = {Modern artificial intelligence provides a novel way of producing digital art in styles. The expressive power of neural networks enables the realm of visual style transfer methods, which can be used to edit images, videos, and 3D data to make them more artistic and diverse. This paper reports on recent advances in neural stylization for 3D data. We provide a taxonomy for neural stylization by considering several important design choices, including scene representation, guidance data, optimization strategies, and output styles. Building on such taxonomy, our survey first revisits the background of neural stylization on 2D images, and then provides in-depth discussions on recent neural stylization methods for 3D data, where we also provide a mini-benchmark on artistic stylization methods. Based on the insights gained from the survey, we then discuss open challenges, future research, and potential applications and impacts of neural stylization.}
}

@article{Chen.2023a,
  title = {{{GaussianEditor}}: {{Swift}} and {{Controllable 3D Editing}} with {{Gaussian Splatting}}},
  shorttitle = {{{GaussianEditor}}},
  author = {Chen, Yiwen and Chen, Zilong and Zhang, Chi and Wang, Feng and Yang, Xiaofeng and Wang, Yikai and Cai, Zhongang and Yang, Lei and Liu, Huaping and Lin, Guosheng},
  date = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2311.14521},
  abstract = {3D editing plays a crucial role in many areas such as gaming and virtual reality. Traditional 3D editing methods, which rely on representations like meshes and point clouds, often fall short in realistically depicting complex scenes. On the other hand, methods based on implicit 3D representations, like Neural Radiance Field (NeRF), render complex scenes effectively but suffer from slow processing speeds and limited control over specific scene areas. In response to these challenges, our paper presents GaussianEditor, an innovative and efficient 3D editing algorithm based on Gaussian Splatting (GS), a novel 3D representation. GaussianEditor enhances precision and control in editing through our proposed Gaussian semantic tracing, which traces the editing target throughout the training process. Additionally, we propose Hierarchical Gaussian splatting (HGS) to achieve stabilized and fine results under stochastic generative guidance from 2D diffusion models. We also develop editing strategies for efficient object removal and integration, a challenging task for existing methods. Our comprehensive experiments demonstrate GaussianEditor's superior control, efficacy, and rapid performance, marking a significant advancement in 3D editing. Project Page: https://buaacyw.github.io/gaussian-editor/},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@misc{Chen.2024,
  title = {{{DGE}}: {{Direct Gaussian 3D Editing}} by {{Consistent Multi-view Editing}}},
  shorttitle = {{{DGE}}},
  author = {Chen, Minghao and Laina, Iro and Vedaldi, Andrea},
  date = {2024-07},
  eprint = {2404.18929},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2404.18929},
  abstract = {We consider the problem of editing 3D objects and scenes based on open-ended language instructions. A common approach to this problem is to use a 2D image generator or editor to guide the 3D editing process, obviating the need for 3D data. However, this process is often inefficient due to the need for iterative updates of costly 3D representations, such as neural radiance fields, either through individual view edits or score distillation sampling. A major disadvantage of this approach is the slow convergence caused by aggregating inconsistent information across views, as the guidance from 2D models is not multi-view consistent. We thus introduce the Direct Gaussian Editor (DGE), a method that addresses these issues in two stages. First, we modify a given high-quality image editor like InstructPix2Pix to be multi-view consistent. To do so, we propose a training-free approach that integrates cues from the 3D geometry of the underlying scene. Second, given a multi-view consistent edited sequence of images, we directly and efficiently optimize the 3D representation, which is based on 3D Gaussian Splatting. Because it avoids incremental and iterative edits, DGE is significantly more accurate and efficient than existing approaches and offers additional benefits, such as enabling selective editing of parts of the scene.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{Darmon.2021,
  title = {Improving {{Neural Implicit Surfaces Geometry}} with {{Patch Warping}}},
  author = {Darmon, François and Bascle, Bénédicte and Devaux, Jean-Clément and Monasse, Pascal and Aubry, Mathieu},
  date = {2021-12},
  abstract = {Neural implicit surfaces have become an important technique for multi-view 3D reconstruction but their accuracy remains limited. In this paper, we argue that this comes from the difficulty to learn and render high frequency textures with neural networks. We thus propose to add to the standard neural rendering optimization a direct photo-consistency term across the different views. Intuitively, we optimize the implicit geometry so that it warps views on each other in a consistent way. We demonstrate that two elements are key to the success of such an approach: (i) warping entire patches, using the predicted occupancy and normals of the 3D points along each ray, and measuring their similarity with a robust structural similarity (SSIM); (ii) handling visibility and occlusion in such a way that incorrect warps are not given too much importance while encouraging a reconstruction as complete as possible. We evaluate our approach, dubbed NeuralWarp, on the standard DTU and EPFL benchmarks and show it outperforms state of the art unsupervised implicit surfaces reconstructions by over 20\% on both datasets.}
}

@misc{Deng.2020,
  title = {Arbitrary {{Style Transfer}} via {{Multi-Adaptation Network}}},
  author = {Deng, Yingying and Tang, Fan and Dong, Weiming and Sun, Wen and Huang, Feiyue and Xu, Changsheng},
  date = {2020},
  abstract = {Arbitrary style transfer is a significant topic with research value and application prospect. A desired style transfer, given a content image and referenced style painting, would render the content image with the color tone and vivid stroke patterns of the style painting while synchronously maintaining the detailed content structure information. Style transfer approaches would initially learn content and style representations of the content and style references and then generate the stylized images guided by these representations. In this paper, we propose the multi-adaptation network which involves two self-adaptation (SA) modules and one co-adaptation (CA) module: the SA modules adaptively disentangle the content and style representations, i.e., content SA module uses position-wise self-attention to enhance content representation and style SA module uses channel-wise self-attention to enhance style representation; the CA module rearranges the distribution of style representation based on content representation distribution by calculating the local similarity between the disentangled content and style features in a non-local fashion. Moreover, a new disentanglement loss function enables our network to extract main style patterns and exact content structures to adapt to various input images, respectively. Various qualitative and quantitative experiments demonstrate that the proposed multi-adaptation network leads to better results than the state-of-the-art style transfer methods.}
}

@misc{Duan.2023,
  title = {{{DiffSynth}}: {{Latent In-Iteration Deflickering}} for {{Realistic Video Synthesis}}},
  shorttitle = {{{DiffSynth}}},
  author = {Duan, Zhongjie and You, Lizhou and Wang, Chengyu and Chen, Cen and Wu, Ziheng and Qian, Weining and Huang, Jun},
  date = {2023-08},
  eprint = {2308.03463},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.03463},
  abstract = {In recent years, diffusion models have emerged as the most powerful approach in image synthesis. However, applying these models directly to video synthesis presents challenges, as it often leads to noticeable flickering contents. Although recently proposed zero-shot methods can alleviate flicker to some extent, we still struggle to generate coherent videos. In this paper, we propose DiffSynth, a novel approach that aims to convert image synthesis pipelines to video synthesis pipelines. DiffSynth consists of two key components: a latent in-iteration deflickering framework and a video deflickering algorithm. The latent in-iteration deflickering framework applies video deflickering to the latent space of diffusion models, effectively preventing flicker accumulation in intermediate steps. Additionally, we propose a video deflickering algorithm, named patch blending algorithm, that remaps objects in different frames and blends them together to enhance video consistency. One of the notable advantages of DiffSynth is its general applicability to various video synthesis tasks, including text-guided video stylization, fashion video synthesis, image-guided video stylization, video restoring, and 3D rendering. In the task of text-guided video stylization, we make it possible to synthesize high-quality videos without cherry-picking. The experimental results demonstrate the effectiveness of DiffSynth. All videos can be viewed on our project page. Source codes will also be released.},
  organization = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia}
}

@article{Dubosc.2021,
  title = {Impact of {{Avatar Facial Anthropomorphism}} on {{Body Ownership}}, {{Attractiveness}} and {{Social Presence}} in {{Collaborative Tasks}} in {{Immersive Virtual Environments}}},
  author = {Dubosc, Charlotte and Gorisse, Geoffrey and Christmann, Olivier and Fleury, Sylvain and Poinsot, Killian and Richir, Simon},
  date = {2021},
  journaltitle = {Computers \& Graphics},
  volume = {101},
  pages = {82--92},
  issn = {0097-8493},
  doi = {10.1016/j.cag.2021.08.011},
  abstract = {Effective collaboration in immersive virtual environments requires to be able to communicate flawlessly using both verbal and non-verbal communication. We present two experiments investigating the impact of facial anthropomorphism on the sense of body ownership, avatar attractiveness, social presence and performance in two collaborative tasks. In the first experiment participants have to solve a construction game according to their partner's instructions using three avatars presenting different facial properties. Results reveal no significant difference in terms of body ownership and social presence, but demonstrate significant differences in terms of attractiveness and completion duration of the collaborative task. Unexpectedly, correlation analyses also reveal a link between attractiveness and performance. The more attractive the avatar, the shorter the completion duration of the game. Our second experiment was designed to investigate further the potential impact of the task carried out on the sense of social presence using the same avatars. While we observed a very high sense of social presence in both tasks (asymmetric collaboration and negotiation) with every avatar, our results did not reveal significant difference between the three conditions. However, we observed statistically significant differences between the two task types. The scores of the co-presence and of the perceived message understanding dimensions of social presence were higher during the negotiation task. The sense of social presence appears to be task sensitive, especially when non-verbal communication becomes more important during face-to-face interaction in immersive collaborative virtual environments.}
}

@misc{Gao.2022,
  title = {{{GET3D}}: {{A Generative Model}} of {{High Quality 3D Textured Shapes Learned}} from {{Images}}},
  author = {Gao, Jun and Shen, Tianchang and Wang, Zian and Chen, Wenzheng and Yin, Kangxue and Li, Daiqing and Litany, Or and Gojcic, Zan and Fidler, Sanja},
  date = {2022},
  abstract = {As several industries are moving towards modeling massive 3D virtual worlds, the need for content creation tools that can scale in terms of the quantity, quality, and diversity of 3D content is becoming evident. In our work, we aim to train performant 3D generative models that synthesize textured meshes which can be directly consumed by 3D rendering engines, thus immediately usable in downstream applications. Prior works on 3D generative modeling either lack geometric details, are limited in the mesh topology they can produce, typically do not support textures, or utilize neural renderers in the synthesis process, which makes their use in common 3D software non-trivial. In this work, we introduce GET3D, a Generative model that directly generates Explicit Textured 3D meshes with complex topology, rich geometric details, and high-fidelity textures. We bridge recent success in the differentiable surface modeling, differentiable rendering as well as 2D Generative Adversarial Networks to train our model from 2D image collections. GET3D is able to generate high-quality 3D textured meshes, ranging from cars, chairs, animals, motorbikes and human characters to buildings, achieving significant improvements over previous methods.}
}

@misc{Gatys.2015,
  title = {A {{Neural Algorithm}} of {{Artistic Style}}},
  author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  date = {2015-09},
  eprint = {1508.06576},
  eprinttype = {arXiv},
  eprintclass = {cs, q-bio},
  doi = {10.48550/arXiv.1508.06576},
  abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
  organization = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition}
}

@inproceedings{Gatys.2016,
  title = {Image {{Style Transfer Using Convolutional Neural Networks}}},
  booktitle = {29th {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  date = {2016},
  pages = {2414--2423},
  publisher = {IEEE},
  location = {Piscataway, NJ},
  isbn = {978-1-4673-8851-1}
}

@misc{Geyer.2023,
  title = {{{TokenFlow}}: {{Consistent Diffusion Features}} for {{Consistent Video Editing}}},
  author = {Geyer, Michal and Bar-Tal, Omer and Bagon, Shai and Dekel, Tali},
  date = {2023},
  abstract = {The generative AI revolution has recently expanded to videos. Nevertheless, current state-of-the-art video models are still lagging behind image models in terms of visual quality and user control over the generated content. In this work, we present a framework that harnesses the power of a text-to-image diffusion model for the task of text-driven video editing. Specifically, given a source video and a target text-prompt, our method generates a high-quality video that adheres to the target text, while preserving the spatial layout and motion of the input video. Our method is based on a key observation that consistency in the edited video can be obtained by enforcing consistency in the diffusion feature space. We achieve this by explicitly propagating diffusion features based on inter-frame correspondences, readily available in the model. Thus, our framework does not require any training or fine-tuning, and can work in conjunction with any off-the-shelf text-to-image editing method. We demonstrate state-of-the-art editing results on a variety of real-world videos. Webpage: https://diffusion-tokenflow.github.io/}
}

@article{Ghiasi.2017,
  title = {Exploring the {{Structure}} of a {{Real-Time}}, {{Arbitrary Neural Artistic Stylization Network}}},
  author = {Ghiasi, Golnaz and Lee, Honglak and Kudlur, Manjunath and Dumoulin, Vincent and Shlens, Jonathon},
  date = {2017},
  doi = {10.5244/C.31.114},
  abstract = {In this paper, we present a method which combines the flexibility of the neural algorithm of artistic style with the speed of fast style transfer networks to allow real-time stylization using any content/style image pair. We build upon recent work leveraging conditional instance normalization for multi-style transfer networks by learning to predict the conditional instance normalization parameters directly from a style image. The model is successfully trained on a corpus of roughly 80,000 paintings and is able to generalize to paintings previously unobserved. We demonstrate that the learned embedding space is smooth and contains a rich structure and organizes semantic information associated with paintings in an entirely unsupervised manner.}
}

@article{Han.2021,
  title = {Exemplar-{{Based 3D Portrait Stylization}}},
  author = {Han, Fangzhou and Ye, Shuquan and He, Mingming and Chai, Menglei and Liao, Jing},
  date = {2021},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {PP},
  number = {2},
  pages = {1371--1383},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2021.3114308},
  abstract = {Exemplar-based portrait stylization is widely attractive and highly desired. Despite recent successes, it remains challenging, especially when considering both texture and geometric styles. In this paper, we present the first framework for one-shot 3D portrait style transfer, which can generate 3D face models with both the geometry exaggerated and the texture stylized while preserving the identity from the original content. It requires only one arbitrary style image instead of a large set of training examples for a particular style, provides geometry and texture outputs that are fully parameterized and disentangled, and enables further graphics applications with the 3D representations. The framework consists of two stages. In the first geometric style transfer stage, we use facial landmark translation to capture the coarse geometry style and guide the deformation of the dense 3D face geometry. In the second texture style transfer stage, we focus on performing style transfer on the canonical texture by adopting a differentiable renderer to optimize the texture in a multi-view framework. Experiments show that our method achieves robustly good results on different artistic styles and outperforms existing methods. We also demonstrate the advantages of our method via various 2D and 3D graphics applications.}
}

@article{Haque.2023,
  title = {Instruct-{{NeRF2NeRF}}: {{Editing 3D Scenes}} with {{Instructions}}},
  author = {Haque, Ayaan and Tancik, Matthew and Efros, Alexei A. and Holynski, Aleksander and Kanazawa, Angjoo},
  date = {2023},
  doi = {10.1109/ICCV51070.2023.01808},
  abstract = {We propose a method for editing NeRF scenes with text-instructions. Given a NeRF of a scene and the collection of images used to reconstruct it, our method uses an image-conditioned diffusion model (InstructPix2Pix) to iteratively edit the input images while optimizing the underlying scene, resulting in an optimized 3D scene that respects the edit instruction. We demonstrate that our proposed method is able to edit large-scale, real-world scenes, and is able to accomplish more realistic, targeted edits than prior work.}
}

@misc{Hertz.2022,
  title = {Prompt-to-{{Prompt Image Editing}} with {{Cross Attention Control}}},
  author = {Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel},
  date = {2022},
  abstract = {Recent large-scale text-driven synthesis models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Such text-based synthesis methods are particularly appealing to humans who are used to verbally describe their intent. Therefore, it is only natural to extend the text-driven image synthesis to text-driven image editing. Editing is challenging for these generative models, since an innate property of an editing technique is to preserve most of the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive prompt-to-prompt editing framework, where the edits are controlled by text only. To this end, we analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we present several applications which monitor the image synthesis by editing the textual prompt only. This includes localized editing by replacing a word, global editing by adding a specification, and even delicately controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts, demonstrating high-quality synthesis and fidelity to the edited prompts.}
}

@misc{Hertz.2023,
  title = {Style {{Aligned Image Generation}} via {{Shared Attention}}},
  author = {Hertz, Amir and Voynov, Andrey and Fruchter, Shlomi and Cohen-Or, Daniel},
  date = {2023-04},
  abstract = {Large-scale Text-to-Image (T2I) models have rapidly gained prominence across creative fields, generating visually compelling outputs from textual prompts. However, controlling these models to ensure consistent style remains challenging, with existing methods necessitating fine-tuning and manual intervention to disentangle content and style. In this paper, we introduce StyleAligned, a novel technique designed to establish style alignment among a series of generated images. By employing minimal `attention sharing' during the diffusion process, our method maintains style consistency across images within T2I models. This approach allows for the creation of style-consistent images using a reference style through a straightforward inversion operation. Our method's evaluation across diverse styles and text prompts demonstrates high-quality synthesis and fidelity, underscoring its efficacy in achieving consistent style across various inputs.}
}

@misc{Heusel.2018,
  title = {{{GANs Trained}} by a {{Two Time-Scale Update Rule Converge}} to a {{Local Nash Equilibrium}}},
  author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  date = {2018-01},
  eprint = {1706.08500},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/1706.08500},
  urldate = {2024-09-11},
  abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the `Fréchet Inception Distance” (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{Hollein.2022,
  title = {{{StyleMesh}}: {{Style Transfer}} for {{Indoor 3D Scene Reconstructions}}},
  author = {Höllein, Lukas and Johnson, Justin and Nießner, Matthias},
  date = {2022},
  pages = {6198--6208}
}

@misc{Hu.2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  date = {2021-10},
  eprint = {2106.09685},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2106.09685},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@online{Huang.2021,
  title = {Learning to {{Stylize Novel Views}}},
  author = {Huang, Hsin-Ping and Tseng, Hung-Yu and Saini, Saurabh and Singh, Maneesh and Yang, Ming-Hsuan},
  date = {2021-09-15},
  eprint = {2105.13509},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2105.13509},
  urldate = {2024-10-27},
  abstract = {We tackle a 3D scene stylization problem - generating stylized images of a scene from arbitrary novel views given a set of images of the same scene and a reference image of the desired style as inputs. Direct solution of combining novel view synthesis and stylization approaches lead to results that are blurry or not consistent across different views. We propose a point cloud-based method for consistent 3D scene stylization. First, we construct the point cloud by back-projecting the image features to the 3D space. Second, we develop point cloud aggregation modules to gather the style information of the 3D scene, and then modulate the features in the point cloud with a linear transformation matrix. Finally, we project the transformed features to 2D space to obtain the novel views. Experimental results on two diverse datasets of real-world scenes validate that our method generates consistent stylized novel view synthesis results against other alternative approaches.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/lucky/Zotero/storage/XPNUEHT6/Huang et al. - 2021 - Learning to Stylize Novel Views.pdf;/home/lucky/Zotero/storage/FCY7BSK2/2105.html}
}

@misc{Ibrahimli.2023,
  title = {{{MuVieCAST}}: {{Multi-view Consistent Artistic Style Transfer}}},
  author = {Ibrahimli, Nail and Kooij, Julian F. P. and Nan, Liangliang},
  date = {2023},
  abstract = {We introduce MuVieCAST, a modular multi-view consistent style transfer network architecture that enables consistent style transfer between multiple viewpoints of the same scene. This network architecture supports both sparse and dense views, making it versatile enough to handle a wide range of multi-view image datasets. The approach consists of three modules that perform specific tasks related to style transfer, namely content preservation, image transformation, and multi-view consistency enforcement. We extensively evaluate our approach across multiple application domains including depth-map-based point cloud fusion, mesh reconstruction, and novel-view synthesis. Our experiments reveal that the proposed framework achieves an exceptional generation of stylized images, exhibiting consistent outcomes across perspectives. A user study focusing on novel-view synthesis further confirms these results, with approximately 68\% of cases participants expressing a preference for our generated outputs compared to the recent state-of-the-art method. Our modular framework is extensible and can easily be integrated with various backbone architectures, making it a flexible solution for multi-view style transfer. More results are demonstrated on our project page: muviecast.github.io}
}

@misc{Jaganathan.2024,
  title = {{{ICE-G}}: {{Image Conditional Editing}} of {{3D Gaussian Splats}}},
  shorttitle = {{{ICE-G}}},
  author = {Jaganathan, Vishnu and Huang, Hannah Hanyun and Irshad, Muhammad Zubair and Jampani, Varun and Raj, Amit and Kira, Zsolt},
  date = {2024-06},
  eprint = {2406.08488},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2406.08488},
  urldate = {2024-09-10},
  abstract = {Recently many techniques have emerged to create high quality 3D assets and scenes. When it comes to editing of these objects, however, existing approaches are either slow, compromise on quality, or do not provide enough customization. We introduce a novel approach to quickly edit a 3D model from a single reference view. Our technique first segments the edit image, and then matches semantically corresponding regions across chosen segmented dataset views using DINO features. A color or texture change from a particular region of the edit image can then be applied to other views automatically in a semantically sensible manner. These edited views act as an updated dataset to further train and re-style the 3D scene. The end-result is therefore an edited 3D model. Our framework enables a wide variety of editing tasks such as manual local edits, correspondence based style transfer from any example image, and a combination of different styles from multiple example images. We use Gaussian Splats as our primary 3D representation due to their speed and ease of local editing, but our technique works for other methods such as NeRFs as well. We show through multiple examples that our method produces higher quality results while offering fine-grained control of editing. Project page: ice-gaussian.github.io},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{Jing.2020,
  title = {Neural {{Style Transfer}}: {{A Review}}},
  author = {Jing, Yongcheng and Yang, Yezhou and Feng, Zunlei and Ye, Jingwen and Yu, Yizhou and Song, Mingli},
  date = {2020},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {26},
  number = {11},
  pages = {3365--3385},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2019.2921336},
  abstract = {The seminal work of Gatys et al. demonstrated the power of Convolutional Neural Networks (CNNs) in creating artistic imagery by separating and recombining image content and style. This process of using CNNs to render a content image in different styles is referred to as Neural Style Transfer (NST). Since then, NST has become a trending topic both in academic literature and industrial applications. It is receiving increasing attention and a variety of approaches are proposed to either improve or extend the original NST algorithm. In this paper, we aim to provide a comprehensive overview of the current progress towards NST. We first propose a taxonomy of current algorithms in the field of NST. Then, we present several evaluation methods and compare different NST algorithms both qualitatively and quantitatively. The review concludes with a discussion of various applications of NST and open problems for future research. A list of papers discussed in this review, corresponding codes, pre-trained models and more comparison results are publicly available at: https://osf.io/f8tu4/.}
}

@misc{Johnson.2016,
  title = {Perceptual {{Losses}} for {{Real-Time Style Transfer}} and {{Super-Resolution}}},
  author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
  date = {2016-03},
  eprint = {1603.08155},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/1603.08155},
  urldate = {2024-09-12},
  abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{Kamata.2023,
  title = {Instruct {{3D-to-3D}}: {{Text Instruction Guided 3D-to-3D Conversion}}},
  author = {Kamata, Hiromichi and Sakuma, Yuiko and Hayakawa, Akio and Ishii, Masato and Narihira, Takuya},
  date = {2023},
  abstract = {We propose a high-quality 3D-to-3D conversion method, Instruct 3D-to-3D. Our method is designed for a novel task, which is to convert a given 3D scene to another scene according to text instructions. Instruct 3D-to-3D applies pretrained Image-to-Image diffusion models for 3D-to-3D conversion. This enables the likelihood maximization of each viewpoint image and high-quality 3D generation. In addition, our proposed method explicitly inputs the source 3D scene as a condition, which enhances 3D consistency and controllability of how much of the source 3D scene structure is reflected. We also propose dynamic scaling, which allows the intensity of the geometry transformation to be adjusted. We performed quantitative and qualitative evaluations and showed that our proposed method achieves higher quality 3D-to-3D conversions than baseline methods.}
}

@article{Kang.2023,
  title = {Neural {{Style Transfer}} for {{3D Meshes}}},
  author = {Kang, Hongyuan and Dong, Xiao and Cao, Juan and Chen, Zhonggui},
  date = {2023},
  journaltitle = {Graphical Models},
  volume = {129},
  pages = {101198},
  issn = {1524-0703},
  doi = {10.1016/j.gmod.2023.101198},
  abstract = {Style transfer is a popular research topic in the field of computer vision. In 3D stylization, a mesh model is deformed to achieve a specific geometric style. We explore a general neural style transfer framework for 3D meshes that can transfer multiple geometric styles from other meshes to the current mesh. Our stylization network is based on a pre-trained MeshNet model, from which content representation and Gram-based style representation are extracted. By constraining the similarity in content and style representation between the generated mesh and two different meshes, our network can generate a deformed mesh with a specific style while maintaining the content of the original mesh. Experiments verify the robustness of the proposed network and show the effectiveness of stylizing multiple models with one dedicated style mesh. We also conduct ablation experiments to analyze the effectiveness of our network.}
}

@misc{Karim.2023,
  title = {Free-{{Editor}}: {{Zero-shot Text-driven 3D Scene Editing}}},
  shorttitle = {Free-{{Editor}}},
  author = {Karim, Nazmul and Iqbal, Hasan and Khalid, Umar and Hua, Jing and Chen, Chen},
  date = {2023-12},
  url = {https://arxiv.org/abs/2312.13663v2},
  urldate = {2024-09-11},
  abstract = {Text-to-Image (T2I) diffusion models have recently gained traction for their versatility and user-friendliness in 2D content generation and editing. However, training a diffusion model specifically for 3D scene editing is challenging due to the scarcity of large-scale datasets. Currently, editing 3D scenes necessitates either retraining the model to accommodate various 3D edits or developing specific methods tailored to each unique editing type. Moreover, state-of-the-art (SOTA) techniques require multiple synchronized edited images from the same scene to enable effective scene editing. Given the current limitations of T2I models, achieving consistent editing effects across multiple images remains difficult, leading to multi-view inconsistency in editing. This inconsistency undermines the performance of 3D scene editing when these images are utilized. In this study, we introduce a novel, training-free 3D scene editing technique called \textbackslash textbackslash textsc\{Free-Editor\}, which enables users to edit 3D scenes without the need for model retraining during the testing phase. Our method effectively addresses the issue of multi-view style inconsistency found in state-of-the-art (SOTA) methods through the implementation of a single-view editing scheme. Specifically, we demonstrate that editing a particular 3D scene can be achieved by modifying only a single view. To facilitate this, we present an Edit Transformer that ensures intra-view consistency and inter-view style transfer using self-view and cross-view attention mechanisms, respectively. By eliminating the need for model retraining and multi-view editing, our approach significantly reduces editing time and memory resource requirements, achieving runtimes approximately 20 times faster than SOTA methods. We have performed extensive experiments on various benchmark datasets, showcasing the diverse editing capabilities of our proposed technique.},
  organization = {arXiv.org}
}

@article{Kato.2018,
  title = {Neural {{3D Mesh Renderer}}},
  author = {Kato, Hiroharu and Ushiku, Yoshitaka and Harada, Tatsuya},
  date = {2018},
  doi = {10.1109/CVPR.2018.00411},
  abstract = {For modeling the 3D world behind 2D images, which 3D representation is most appropriate? A polygon mesh is a promising candidate for its compactness and geometric properties. However, it is not straightforward to model a polygon mesh from 2D images using neural networks because the conversion from a mesh to an image, or rendering, involves a discrete operation called rasterization, which prevents back-propagation. Therefore, in this work, we propose an approximate gradient for rasterization that enables the integration of rendering into neural networks. Using this renderer, we perform single-image 3D mesh reconstruction with silhouette image supervision and our system outperforms the existing voxel-based approach. Additionally, we perform gradient-based 3D mesh editing operations, such as 2D-to-3D style transfer and 3D DeepDream, with 2D supervision for the first time. These applications demonstrate the potential of the integration of a mesh renderer into neural networks and the effectiveness of our proposed renderer.}
}

@misc{Kato.2020,
  title = {Differentiable {{Rendering}}: {{A Survey}}},
  author = {Kato, Hiroharu and Beker, Deniz and Morariu, Mihai and Ando, Takahiro and Matsuoka, Toru and Kehl, Wadim and Gaidon, Adrien},
  date = {2020-06},
  abstract = {Deep neural networks (DNNs) have shown remarkable performance improvements on vision-related tasks such as object detection or image segmentation. Despite their success, they generally lack the understanding of 3D objects which form the image, as it is not always possible to collect 3D information about the scene or to easily annotate it. Differentiable rendering is a novel field which allows the gradients of 3D objects to be calculated and propagated through images. It also reduces the requirement of 3D data collection and annotation, while enabling higher success rate in various applications. This paper reviews existing literature and discusses the current state of differentiable rendering, its applications and open research problems.}
}

@article{Kerbl.2023,
  title = {{{3D Gaussian Splatting}} for {{Real-Time Radiance Field Rendering}}},
  author = {Kerbl, Bernhard and Kopanas, Georgios and Leimkühler, Thomas and Drettakis, George},
  date = {2023},
  journaltitle = {ACM Transactions on Graphics},
  doi = {10.1145/3592433},
  abstract = {Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time ({$>$}= 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.}
}

@misc{Kerdreux.2020,
  title = {Interactive {{Neural Style Transfer}} with {{Artists}}},
  author = {Kerdreux, Thomas and Thiry, Louis and Kerdreux, Erwan},
  date = {2020-03},
  eprint = {2003.06659},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2003.06659},
  abstract = {We present interactive painting processes in which a painter and various neural style transfer algorithms interact on a real canvas. Understanding what these algorithms' outputs achieve is then paramount to describe the creative agency in our interactive experiments. We gather a set of paired painting-pictures images and present a new evaluation methodology based on the predictivity of neural style transfer algorithms. We point some algorithms' instabilities and show that they can be used to enlarge the diversity and pleasing oddity of the images synthesized by the numerous existing neural style transfer algorithms. This diversity of images was perceived as a source of inspiration for human painters, portraying the machine as a computational catalyst.},
  organization = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning}
}

@article{Kim.2020,
  title = {Deformable {{Style Transfer}}},
  author = {Kim, Sunnie S. Y. and Kolkin, Nicholas and Salavon, Jason and Shakhnarovich, Gregory},
  date = {2020},
  doi = {10.1007/978-3-030-58574-7_15},
  abstract = {Both geometry and texture are fundamental aspects of visual style. Existing style transfer methods, however, primarily focus on texture, almost entirely ignoring geometry. We propose deformable style transfer (DST), an optimization-based approach that jointly stylizes the texture and geometry of a content image to better match a style image. Unlike previous geometry-aware stylization methods, our approach is neither restricted to a particular domain (such as human faces), nor does it require training sets of matching style/content pairs. We demonstrate our method on a diverse set of content and style images including portraits, animals, objects, scenes, and paintings. Code has been made publicly available at https://github.com/sunniesuhyoung/DST.}
}

@inproceedings{Kim.2022,
  title = {Revisiting {{Image Pyramid Structure}} for {{High Resolution Salient Object Detection}}},
  author = {Kim, Taehun and Kim, Kunhee and Lee, Joonyeong and Cha, Dongmin and Lee, Jiho and Kim, Daijin},
  date = {2022},
  pages = {108--124},
  url = {https://openaccess.thecvf.com/content/ACCV2022/html/Kim_Revisiting_Image_Pyramid_Structure_for_High_Resolution_Salient_Object_Detection_ACCV_2022_paper.html},
  urldate = {2024-10-15},
  eventtitle = {Proceedings of the {{Asian Conference}} on {{Computer Vision}}},
  langid = {english},
  file = {/home/lucky/Zotero/storage/B68NRSVL/Kim et al. - 2022 - Revisiting Image Pyramid Structure for High Resolution Salient Object Detection.pdf}
}

@article{Kolkin.2019,
  title = {Style {{Transfer}} by {{Relaxed Optimal Transport}} and {{Self-Similarity}}},
  author = {Kolkin, Nicholas and Salavon, Jason and Shakhnarovich, Greg},
  date = {2019},
  doi = {10.1109/CVPR.2019.01029},
  abstract = {Style transfer algorithms strive to render the content of one image using the style of another. We propose Style Transfer by Relaxed Optimal Transport and Self-Similarity (STROTSS), a new optimization-based style transfer algorithm. We extend our method to allow user-specified point-to-point or region-to-region control over visual similarity between the style image and the output. Such guidance can be used to either achieve a particular visual effect or correct errors made by unconstrained style transfer. In order to quantitatively compare our method to prior work, we conduct a large-scale user study designed to assess the style-content tradeoff across settings in style transfer algorithms. Our results indicate that for any desired level of content preservation, our method provides higher quality stylization than prior work. Code is available at https://github.com/nkolkin13/STROTSS}
}

@article{Kyrlitsias.2022,
  title = {Social {{Interaction}} with {{Agents}} and {{Avatars}} in {{Immersive Virtual Environments}}: {{A Survey}}},
  author = {Kyrlitsias, Christos and Michael-Grigoriou, Despina},
  date = {2022},
  journaltitle = {Frontiers in Virtual Reality},
  volume = {2},
  pages = {786665},
  doi = {10.3389/frvir.2021.786665},
  abstract = {Immersive virtual reality technologies are used in a wide range of fields such as training, education, health, and research. Many of these applications are including virtual humans which and are classified into avatars and agents. An overview of the applications and the advantages of immersive virtual reality and virtual humans is presented in this survey, as well as the basic concepts and terminology. In order to be effective, many virtual reality applications require that the users perceive and react socially to the virtual humans in a realistic manner. Numerous studies show that people can react socially to virtual humans, however, this is not always the case. This paper provides an overview of the main findings regarding the factors that affecting the social interaction with virtual humans within immersive virtual environments. Finally, this paper highlights the need for further research that can lead to better understanding of human-virtual human interaction.}
}

@misc{Li.2024,
  title = {Advances in {{3D Generation}}: {{A Survey}}},
  author = {Li, Xiaoyu and Zhang, Qi and {Di Kang} and Cheng, Weihao and Gao, Yiming and Zhang, Jingbo and Liang, Zhihao and Liao, Jing and Cao, Yan-Pei and Shan, Ying},
  date = {2024},
  abstract = {Generating 3D models lies at the core of computer graphics and has been the focus of decades of research. With the emergence of advanced neural representations and generative models, the field of 3D content generation is developing rapidly, enabling the creation of increasingly high-quality and diverse 3D models. The rapid growth of this field makes it difficult to stay abreast of all recent developments. In this survey, we aim to introduce the fundamental methodologies of 3D generation methods and establish a structured roadmap, encompassing 3D representation, generation methods, datasets, and corresponding applications. Specifically, we introduce the 3D representations that serve as the backbone for 3D generation. Furthermore, we provide a comprehensive overview of the rapidly growing literature on generation methods, categorized by the type of algorithmic paradigms, including feedforward generation, optimization-based generation, procedural generation, and generative novel view synthesis. Lastly, we discuss available datasets, applications, and open challenges. We hope this survey will help readers explore this exciting topic and foster further advancements in the field of 3D content generation.}
}

@misc{Liu.2019,
  title = {Soft {{Rasterizer}}: {{A Differentiable Renderer}} for {{Image-Based 3D Reasoning}}},
  author = {Liu, Shichen and Li, Tianye and Chen, Weikai and Li, Hao},
  date = {2019},
  abstract = {Rendering bridges the gap between 2D vision and 3D scenes by simulating the physical process of image formation. By inverting such renderer, one can think of a learning approach to infer 3D information from 2D images. However, standard graphics renderers involve a fundamental discretization step called rasterization, which prevents the rendering process to be differentiable, hence able to be learned. Unlike the state-of-the-art differentiable renderers, which only approximate the rendering gradient in the back propagation, we propose a truly differentiable rendering framework that is able to (1) directly render colorized mesh using differentiable functions and (2) back-propagate efficient supervision signals to mesh vertices and their attributes from various forms of image representations, including silhouette, shading and color images. The key to our framework is a novel formulation that views rendering as an aggregation function that fuses the probabilistic contributions of all mesh triangles with respect to the rendered pixels. Such formulation enables our framework to flow gradients to the occluded and far-range vertices, which cannot be achieved by the previous state-of-the-arts. We show that by using the proposed renderer, one can achieve significant improvement in 3D unsupervised single-view reconstruction both qualitatively and quantitatively. Experiments also demonstrate that our approach is able to handle the challenging tasks in image-based shape fitting, which remain nontrivial to existing differentiable renderers.}
}

@misc{Liu.2023,
  title = {One-2-3-45++: {{Fast Single Image}} to {{3D Objects}} with {{Consistent Multi-View Generation}} and {{3D Diffusion}}},
  author = {Liu, Minghua and Shi, Ruoxi and Chen, Linghao and Zhang, Zhuoyang and Xu, Chao and Wei, Xinyue and Chen, Hansheng and Zeng, Chong and Gu, Jiayuan and Su, Hao},
  date = {2023},
  abstract = {Recent advancements in open-world 3D object generation have been remarkable, with image-to-3D methods offering superior fine-grained control over their text-to-3D counterparts. However, most existing models fall short in simultaneously providing rapid generation speeds and high fidelity to input images - two features essential for practical applications. In this paper, we present One-2-3-45++, an innovative method that transforms a single image into a detailed 3D textured mesh in approximately one minute. Our approach aims to fully harness the extensive knowledge embedded in 2D diffusion models and priors from valuable yet limited 3D data. This is achieved by initially finetuning a 2D diffusion model for consistent multi-view image generation, followed by elevating these images to 3D with the aid of multi-view conditioned 3D native diffusion models. Extensive experimental evaluations demonstrate that our method can produce high-quality, diverse 3D assets that closely mirror the original input image. Our project webpage: https://sudo-ai-3d.github.io/One2345plus\_page.}
}

@misc{Liu.2023a,
  title = {{{StyleRF}}: {{Zero-shot 3D Style Transfer}} of {{Neural Radiance Fields}}},
  author = {Liu, Kunhao and Zhan, Fangneng and Chen, Yiwen and Zhang, Jiahui and Yu, Yingchen and Saddik, Abdulmotaleb El and Lu, Shijian and Xing, Eric},
  date = {2023},
  abstract = {3D style transfer aims to render stylized novel views of a 3D scene with multi-view consistency. However, most existing work suffers from a three-way dilemma over accurate geometry reconstruction, high-quality stylization, and being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes, with which high-fidelity geometry can be reliably restored via volume rendering. In addition, it transforms the grid features according to the reference style which directly leads to high-quality zero-shot style transfer. StyleRF consists of two innovative designs. The first is sampling-invariant content transformation that makes the transformation invariant to the holistic statistics of the sampled 3D points and accordingly ensures multi-view consistency. The second is deferred style transformation of 2D feature maps which is equivalent to the transformation of 3D points but greatly reduces memory footprint without degrading multi-view consistency. Extensive experiments show that StyleRF achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner.}
}

@misc{Liu.2024,
  title = {{{StyleGaussian}}: {{Instant 3D Style Transfer}} with {{Gaussian Splatting}}},
  author = {Liu, Kunhao and Zhan, Fangneng and Xu, Muyu and Theobalt, Christian and Shao, Ling and Lu, Shijian},
  date = {2024},
  abstract = {We introduce StyleGaussian, a novel 3D style transfer technique that allows instant transfer of any image's style to a 3D scene at 10 frames per second (fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves style transfer without compromising its real-time rendering ability and multi-view consistency. It achieves instant style transfer with three steps: embedding, transfer, and decoding. Initially, 2D VGG scene features are embedded into reconstructed 3D Gaussians. Next, the embedded features are transformed according to a reference style image. Finally, the transformed features are decoded into the stylized RGB. StyleGaussian has two novel designs. The first is an efficient feature rendering strategy that first renders low-dimensional features and then maps them into high-dimensional features while embedding VGG features. It cuts the memory consumption significantly and enables 3DGS to render the high-dimensional memory-intensive features. The second is a K-nearest-neighbor-based 3D CNN. Working as the decoder for the stylized features, it eliminates the 2D CNN operations that compromise strict multi-view consistency. Extensive experiments show that StyleGaussian achieves instant 3D stylization with superior stylization quality while preserving real-time rendering and strict multi-view consistency. Project page: https://kunhao-liu.github.io/StyleGaussian/}
}

@article{Lorensen.1987,
  title = {Marching {{Cubes}}: {{A High Resolution 3D Surface Construction Algorithm}}},
  author = {Lorensen, William E. and Cline, Harvey E.},
  date = {1987},
  journaltitle = {ACM SIGGRAPH Computer Graphics},
  volume = {21},
  number = {4},
  pages = {163--169},
  issn = {0097-8930},
  doi = {10.1145/37402.37422}
}

@misc{Luo.2021,
  title = {Diffusion {{Probabilistic Models}} for {{3D Point Cloud Generation}}},
  author = {Luo, Shitong and Hu, Wei},
  date = {2021},
  abstract = {We present a probabilistic model for point cloud generation, which is fundamental for various 3D vision tasks such as shape completion, upsampling, synthesis and data augmentation. Inspired by the diffusion process in non-equilibrium thermodynamics, we view points in point clouds as particles in a thermodynamic system in contact with a heat bath, which diffuse from the original distribution to a noise distribution. Point cloud generation thus amounts to learning the reverse diffusion process that transforms the noise distribution to the distribution of a desired shape. Specifically, we propose to model the reverse diffusion process for point clouds as a Markov chain conditioned on certain shape latent. We derive the variational bound in closed form for training and provide implementations of the model. Experimental results demonstrate that our model achieves competitive performance in point cloud generation and auto-encoding. The code is available at {$<$}a href="https://github.com/luost26/diffusion-point-cloud"{$>$}https://github.com/luost26/diffusion-point-cloud{$<$}/a{$>$}.}
}

@article{Luo.2023,
  title = {{{RaBit}}: {{Parametric Modeling}} of {{3D Biped Cartoon Characters}} with a {{Topological-Consistent Dataset}}},
  author = {Luo, Zhongjin and Cai, Shengcai and Dong, Jinguo and Ming, Ruibo and Qiu, Liangdong and Zhan, Xiaohang and Han, Xiaoguang},
  date = {2023},
  doi = {10.1109/CVPR52729.2023.01233},
  abstract = {Assisting people in efficiently producing visually plausible 3D characters has always been a fundamental research topic in computer vision and computer graphics. Recent learning-based approaches have achieved unprecedented accuracy and efficiency in the area of 3D real human digitization. However, none of the prior works focus on modeling 3D biped cartoon characters, which are also in great demand in gaming and filming. In this paper, we introduce 3DBiCar, the first large-scale dataset of 3D biped cartoon characters, and RaBit, the corresponding parametric model. Our dataset contains 1,500 topologically consistent high-quality 3D textured models which are manually crafted by professional artists. Built upon the data, RaBit is thus designed with a SMPL-like linear blend shape model and a StyleGAN-based neural UV-texture generator, simultaneously expressing the shape, pose, and texture. To demonstrate the practicality of 3DBiCar and RaBit, various applications are conducted, including single-view reconstruction, sketch-based modeling, and 3D cartoon animation. For the single-view reconstruction setting, we find a straightforward global mapping from input images to the output UV-based texture maps tends to lose detailed appearances of some local parts (e.g., nose, ears). Thus, a part-sensitive texture reasoner is adopted to make all important local areas perceived. Experiments further demonstrate the effectiveness of our method both qualitatively and quantitatively. 3DBiCar and RaBit are available at gaplab.cuhk.edu.cn/projects/RaBit.}
}

@article{Mendiratta.2023,
  title = {{{AvatarStudio}}: {{Text-Driven Editing}} of {{3D Dynamic Human Head Avatars}}},
  shorttitle = {{{AvatarStudio}}},
  author = {Mendiratta, Mohit and Pan, Xingang and Elgharib, Mohamed and Teotia, Kartik and R, Mallikarjun B and Tewari, Ayush and Golyanik, Vladislav and Kortylewski, Adam and Theobalt, Christian},
  date = {2023-12},
  journaltitle = {ACM Transactions on Graphics},
  volume = {42},
  number = {6},
  pages = {1--18},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3618368},
  abstract = {Capturing and editing full-head performances enables the creation of virtual characters with various applications such as extended reality and media production. The past few years witnessed a steep rise in the photorealism of human head avatars. Such avatars can be controlled through different input data modalities, including RGB, audio, depth, IMUs, and others. While these data modalities provide effective means of control, they mostly focus on editing the head movements such as the facial expressions, head pose, and/or camera viewpoint. In this paper, we propose AvatarStudio, a text-based method for editing the appearance of a dynamic full head avatar. Our approach builds on existing work to capture dynamic performances of human heads using Neural Radiance Field (NeRF) and edits this representation with a text-to-image diffusion model. Specifically, we introduce an optimization strategy for incorporating multiple keyframes representing different camera viewpoints and time stamps of a video performance into a single diffusion model. Using this personalized diffusion model, we edit the dynamic NeRF by introducing view-and-time-aware Score Distillation Sampling (VT-SDS) following a model-based guidance approach. Our method edits the full head in a canonical space and then propagates these edits to the remaining time steps via a pre-trained deformation network. We evaluate our method visually and numerically via a user study, and results show that our method outperforms existing approaches. Our experiments validate the design choices of our method and highlight that our edits are genuine, personalized, as well as 3D- and time-consistent.}
}

@misc{Michel.2021,
  title = {{{Text2Mesh}}: {{Text-driven Neural Stylization}} for {{Meshes}}},
  author = {Michel, Oscar and Bar-On, Roi and Liu, Richard and Benaim, Sagie and Hanocka, Rana},
  date = {2021-06},
  abstract = {In this work, we develop intuitive controls for editing the style of 3D objects. Our framework, Text2Mesh, stylizes a 3D mesh by predicting color and local geometric details which conform to a target text prompt. We consider a disentangled representation of a 3D object using a fixed mesh input (content) coupled with a learned neural network, which we term neural style field network. In order to modify style, we obtain a similarity score between a text prompt (describing style) and a stylized mesh by harnessing the representational power of CLIP. Text2Mesh requires neither a pre-trained generative model nor a specialized 3D mesh dataset. It can handle low-quality meshes (non-manifold, boundaries, etc.) with arbitrary genus, and does not require UV parameterization. We demonstrate the ability of our technique to synthesize a myriad of styles over a wide variety of 3D meshes.}
}

@misc{Mildenhall.2020,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  date = {2020},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \textbackslash (x,y,z)\textbackslash{} and viewing direction \textbackslash (θ, \textbackslash textbackslash phi)\textbackslash ) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.}
}

@online{Mu.2021,
  title = {{{3D Photo Stylization}}: {{Learning}} to {{Generate Stylized Novel Views}} from a {{Single Image}}},
  shorttitle = {{{3D Photo Stylization}}},
  author = {Mu, Fangzhou and Wang, Jian and Wu, Yicheng and Li, Yin},
  date = {2021-12-04},
  eprint = {2112.00169},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2112.00169},
  urldate = {2024-10-23},
  abstract = {Visual content creation has spurred a soaring interest given its applications in mobile photography and AR / VR. Style transfer and single-image 3D photography as two representative tasks have so far evolved independently. In this paper, we make a connection between the two, and address the challenging task of 3D photo stylization - generating stylized novel views from a single image given an arbitrary style. Our key intuition is that style transfer and view synthesis have to be jointly modeled for this task. To this end, we propose a deep model that learns geometry-aware content features for stylization from a point cloud representation of the scene, resulting in high-quality stylized images that are consistent across views. Further, we introduce a novel training protocol to enable the learning using only 2D images. We demonstrate the superiority of our method via extensive qualitative and quantitative studies, and showcase key applications of our method in light of the growing demand for 3D content creation from 2D image assets.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics}
}

@online{Mu.2021a,
  title = {{{3D Photo Stylization}}: {{Learning}} to {{Generate Stylized Novel Views}} from a {{Single Image}}},
  shorttitle = {{{3D Photo Stylization}}},
  author = {Mu, Fangzhou and Wang, Jian and Wu, Yicheng and Li, Yin},
  date = {2021-12-04},
  eprint = {2112.00169},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2112.00169},
  url = {http://arxiv.org/abs/2112.00169},
  urldate = {2024-10-23},
  abstract = {Visual content creation has spurred a soaring interest given its applications in mobile photography and AR / VR. Style transfer and single-image 3D photography as two representative tasks have so far evolved independently. In this paper, we make a connection between the two, and address the challenging task of 3D photo stylization - generating stylized novel views from a single image given an arbitrary style. Our key intuition is that style transfer and view synthesis have to be jointly modeled for this task. To this end, we propose a deep model that learns geometry-aware content features for stylization from a point cloud representation of the scene, resulting in high-quality stylized images that are consistent across views. Further, we introduce a novel training protocol to enable the learning using only 2D images. We demonstrate the superiority of our method via extensive qualitative and quantitative studies, and showcase key applications of our method in light of the growing demand for 3D content creation from 2D image assets.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics}
}

@article{Mu.2021b,
  title = {{{3D Photo Stylization}}: {{Learning}} to {{Generate Stylized Novel Views}} from a {{Single Image}}},
  shorttitle = {{{3D Photo Stylization}}},
  author = {Mu, Fangzhou and Wang, Jian and Wu, Yicheng and Li, Yin},
  date = {2021},
  journaltitle = {CoRR},
  volume = {abs/2112.00169},
  eprint = {2112.00169},
  eprinttype = {arXiv},
  url = {https://arxiv.org/abs/2112.00169},
  urldate = {2024-10-23}
}

@article{Nguyen-Phuoc.2023,
  title = {{{AlteredAvatar}}: {{Stylizing Dynamic 3D Avatars}} with {{Fast Style Adaptation}}},
  shorttitle = {{{AlteredAvatar}}},
  author = {Nguyen-Phuoc, Thu and Schwartz, Gabriel and Ye, Yuting and Lombardi, Stephen and Xiao, Lei},
  date = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2305.19245},
  abstract = {This paper presents a method that can quickly adapt dynamic 3D avatars to arbitrary text descriptions of novel styles. Among existing approaches for avatar stylization, direct optimization methods can produce excellent results for arbitrary styles but they are unpleasantly slow. Furthermore, they require redoing the optimization process from scratch for every new input. Fast approximation methods using feed-forward networks trained on a large dataset of style images can generate results for new inputs quickly, but tend not to generalize well to novel styles and fall short in quality. We therefore investigate a new approach, AlteredAvatar, that combines those two approaches using the meta-learning framework. In the inner loop, the model learns to optimize to match a single target style well; while in the outer loop, the model learns to stylize efficiently across many styles. After training, AlteredAvatar learns an initialization that can quickly adapt within a small number of update steps to a novel style, which can be given using texts, a reference image, or a combination of both. We show that AlteredAvatar can achieve a good balance between speed, flexibility and quality, while maintaining consistency across a wide range of novel views and facial expressions.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@misc{Nichol.2022,
  title = {Point-{{E}}: {{A System}} for {{Generating 3D Point Clouds}} from {{Complex Prompts}}},
  shorttitle = {Point-{{E}}},
  author = {Nichol, Alex and Jun, Heewoo and Dhariwal, Prafulla and Mishkin, Pamela and Chen, Mark},
  date = {2022-12},
  eprint = {2212.08751},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.08751},
  abstract = {While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at https://github.com/openai/point-e.},
  organization = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{Nilsson.2020,
  title = {Understanding {{SSIM}}},
  author = {Nilsson, Jim and Akenine-Möller, Tomas},
  date = {2020-06},
  eprint = {2006.13846},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  abstract = {The use of the structural similarity index (SSIM) is widespread. For almost two decades, it has played a major role in image quality assessment in many different research disciplines. Clearly, its merits are indisputable in the research community. However, little deep scrutiny of this index has been performed. Contrary to popular belief, there are some interesting properties of SSIM that merit such scrutiny. In this paper, we analyze the mathematical factors of SSIM and show that it can generate results, in both synthetic and realistic use cases, that are unexpected, sometimes undefined, and nonintuitive. As a consequence, assessing image quality based on SSIM can lead to incorrect conclusions and using SSIM as a loss function for deep learning can guide neural network training in the wrong direction.},
  langid = {english},
  organization = {arXiv},
  keywords = {C.4,Computer Science - Graphics,Electrical Engineering and Systems Science - Image and Video Processing,I.3.6}
}

@article{Nowak.2018,
  title = {Avatars and {{Computer-Mediated Communication}}: {{A Review}} of the {{Definitions}}, {{Uses}}, and {{Effects}} of {{Digital Representations}} on {{Communication}}},
  author = {Nowak, Kristine L. and Fox, Jesse},
  date = {2018},
  journaltitle = {Review of Communication Research},
  volume = {6},
  pages = {30--53},
  issn = {2255-4165},
  doi = {10.12840/issn.2255-4165.2018.06.01.015},
  abstract = {Avatars are growing in popularity and present in many interfaces used for computer-mediated communication (CMC) including social media, e-commerce, and education. Communication researchers have been investigating avatars for over twenty years, and an examination of this literature reveals similarities but also notable discrepancies in conceptual definitions. The goal of this review is to provide a general overview of current debates, methodological approaches, and trends in findings. Our review synthesizes previous research in four areas. First, we examine how scholars have conceptualized the term “avatar,” identify similarities and differences across these definitions, and recommend that scholars use the term consistently. Next, we review theoretical perspectives relevant to avatar perception (e.g., the computers as social actors framework). Then, we examine avatar characteristics that communicators use to discern the humanity and social potential of an avatar (anthropomorphism, form realism, behavioral realism, and perceived agency) and discuss implications for attributions and communication outcomes. We also review findings on the social categorization of avatars, such as when people apply categories like sex, gender, race, and ethnicity to their evaluations of digital representations. Finally, we examine research on avatar selection and design relevant to communication outcomes. Here, we review both motivations in CMC contexts (such as self-presentation and identity expression) and potential effects (e.g., persuasion). We conclude with a discussion of future directions for avatar research and propose that communication researchers consider avatars not just as a topic of study, but also as a tool for testing theories and understanding critical elements of human communication. Avatar mediated environments provide researchers with a number of advantageous technological affordances that can enable manipulations that may be difficult or inadvisable to execute in natural environments. We conclude by discussing the use of avatar research to extend communication theory and our understanding of communication processes.}
}

@misc{Pang.2021,
  title = {Image-to-{{Image Translation}}: {{Methods}} and {{Applications}}},
  author = {Pang, Yingxue and Lin, Jianxin and Qin, Tao and Chen, Zhibo},
  date = {2021},
  abstract = {Image-to-image translation (I2I) aims to transfer images from a source domain to a target domain while preserving the content representations. I2I has drawn increasing attention and made tremendous progress in recent years because of its wide range of applications in many computer vision and image processing problems, such as image synthesis, segmentation, style transfer, restoration, and pose estimation. In this paper, we provide an overview of the I2I works developed in recent years. We will analyze the key techniques of the existing I2I works and clarify the main progress the community has made. Additionally, we will elaborate on the effect of I2I on the research and industry community and point out remaining challenges in related fields.}
}

@misc{Patashnik.2024,
  title = {Consolidating {{Attention Features}} for {{Multi-View Image Editing}}},
  author = {Patashnik, Or and Gal, Rinon and Cohen-Or, Daniel and Zhu, Jun-Yan and La Torre, Fernando De},
  date = {2024},
  abstract = {Large-scale text-to-image models enable a wide range of image editing techniques, using text prompts or even spatial controls. However, applying these editing methods to multi-view images depicting a single scene leads to 3D-inconsistent results. In this work, we focus on spatial control-based geometric manipulations and introduce a method to consolidate the editing process across various views. We build on two insights: (1) maintaining consistent features throughout the generative process helps attain consistency in multi-view editing, and (2) the queries in self-attention layers significantly influence the image structure. Hence, we propose to improve the geometric consistency of the edited images by enforcing the consistency of the queries. To do so, we introduce QNeRF, a neural radiance field trained on the internal query features of the edited images. Once trained, QNeRF can render 3D-consistent queries, which are then softly injected back into the self-attention layers during generation, greatly improving multi-view consistency. We refine the process through a progressive, iterative method that better consolidates queries across the diffusion timesteps. We compare our method to a range of existing techniques and demonstrate that it can achieve better multi-view consistency and higher fidelity to the input scene. These advantages allow us to train NeRFs with fewer visual artifacts, that are better aligned with the target geometry.}
}

@article{Pereira.2020,
  title = {Efficient {{CIEDE2000-Based Color Similarity Decision}} for {{Computer Vision}}},
  author = {Pereira, Américo and Carvalho, Pedro and Coelho, Gil and Côrte-Real, Luís},
  date = {2020-07},
  journaltitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {30},
  number = {7},
  pages = {2141--2154},
  issn = {1558-2205},
  doi = {10.1109/TCSVT.2019.2914969},
  url = {https://ieeexplore.ieee.org/document/8708282},
  urldate = {2024-10-29},
  abstract = {Color and color differences are critical aspects in many image processing and computer vision applications. A paradigmatic example is object segmentation, where color distances can greatly influence the performance of the algorithms. Metrics for color difference have been proposed in the literature, including the definition of standards such as CIEDE2000, which quantifies the change in visual perception of two given colors. This standard has been recommended for industrial computer vision applications, but the benefits of its application have been impaired by the complexity of the formula. This paper proposes a new strategy that improves the usability of the CIEDE2000 metric when a maximum acceptable distance can be imposed. We argue that, for applications where a maximum value, above which colors are considered to be different, can be established, then it is possible to reduce the amount of calculations of the metric, by preemptively analyzing the color features. This methodology encompasses the benefits of the metric while overcoming its computational limitations, thus broadening the range of applications of CIEDE2000 in both the computer vision algorithms and computational resource requirements.},
  eventtitle = {{{IEEE Transactions}} on {{Circuits}} and {{Systems}} for {{Video Technology}}},
  keywords = {CIEDE2000,Color,color similarity,Computer vision,Dentistry,Image color analysis,Measurement,Minerals,segmentation,Standards},
  file = {/home/lucky/Zotero/storage/8S6K5CNK/Pereira et al. - 2020 - Efficient CIEDE2000-Based Color Similarity Decision for Computer Vision.pdf;/home/lucky/Zotero/storage/MA99YVX5/8708282.html}
}

@misc{Poole.2022,
  title = {{{DreamFusion}}: {{Text-to-3D}} Using {{2D Diffusion}}},
  shorttitle = {{{DreamFusion}}},
  author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},
  date = {2022-09},
  eprint = {2209.14988},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2209.14988},
  abstract = {Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.},
  organization = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{Radford.2021,
  title = {Learning {{Transferable Visual Models}} from {{Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.}
}

@misc{Ravi.2020,
  title = {Accelerating {{3D Deep Learning}} with {{PyTorch3D}}},
  author = {Ravi, Nikhila and Reizenstein, Jeremy and Novotny, David and Gordon, Taylor and Lo, Wan-Yen and Johnson, Justin and Gkioxari, Georgia},
  date = {2020-07},
  eprint = {2007.08501},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2007.08501},
  abstract = {Deep learning has significantly improved 2D image recognition. Extending into 3D may advance many new applications including autonomous vehicles, virtual and augmented reality, authoring 3D content, and even improving 2D recognition. However despite growing interest, 3D deep learning remains relatively underexplored. We believe that some of this disparity is due to the engineering challenges involved in 3D deep learning, such as efficiently processing heterogeneous data and reframing graphics operations to be differentiable. We address these challenges by introducing PyTorch3D, a library of modular, efficient, and differentiable operators for 3D deep learning. It includes a fast, modular differentiable renderer for meshes and point clouds, enabling analysis-by-synthesis approaches. Compared with other differentiable renderers, PyTorch3D is more modular and efficient, allowing users to more easily extend it while also gracefully scaling to large meshes and images. We compare the PyTorch3D operators and renderer with other implementations and demonstrate significant speed and memory improvements. We also use PyTorch3D to improve the state-of-the-art for unsupervised 3D mesh and point cloud prediction from 2D images on ShapeNet. PyTorch3D is open-source and we hope it will help accelerate research in 3D deep learning.},
  organization = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning}
}

@misc{Richardson.2023,
  title = {{{TEXTure}}: {{Text-guided Texturing}} of {{3D Shapes}}},
  author = {Richardson, Elad and Metzer, Gal and Alaluf, Yuval and Giryes, Raja and Cohen-Or, Daniel},
  date = {2023},
  abstract = {In this paper, we present TEXTure, a novel method for text-guided generation, editing, and transfer of textures for 3D shapes. Leveraging a pretrained depth-to-image diffusion model, TEXTure applies an iterative scheme that paints a 3D model from different viewpoints. Yet, while depth-to-image models can create plausible textures from a single viewpoint, the stochastic nature of the generation process can cause many inconsistencies when texturing an entire 3D object. To tackle these problems, we dynamically define a trimap partitioning of the rendered image into three progression states, and present a novel elaborated diffusion sampling process that uses this trimap representation to generate seamless textures from different views. We then show that one can transfer the generated texture maps to new 3D geometries without requiring explicit surface-to-surface mapping, as well as extract semantic textures from a set of images without requiring any explicit reconstruction. Finally, we show that TEXTure can be used to not only generate new textures but also edit and refine existing textures using either a text prompt or user-provided scribbles. We demonstrate that our TEXTuring method excels at generating, transferring, and editing textures through extensive evaluation, and further close the gap between 2D image generation and 3D texturing.}
}

@misc{Rombach.2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
  date = {2022-04},
  eprint = {2112.10752},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2112.10752},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  organization = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{Rosin.2022,
  title = {{{NPRportrait}} 1.0: {{A Three-Level Benchmark}} for {{Non-Photorealistic Rendering}} of {{Portraits}}},
  author = {Rosin, Paul L. and Lai, Yu-Kun and Mould, David and Yi, Ran and Berger, Itamar and Doyle, Lars and Lee, Seungyong and Li, Chuan and Liu, Yong-Jin and Semmo, Amir and Shamir, Ariel and Son, Minjung and Winnemöller, Holger},
  date = {2022},
  journaltitle = {Computational Visual Media},
  volume = {8},
  number = {3},
  pages = {445--465},
  issn = {2096-0433},
  doi = {10.1007/s41095-021-0255-3}
}

@misc{Segu.2020,
  title = {{{3DSNet}}: {{Unsupervised Shape-to-Shape 3D Style Transfer}}},
  author = {Segu, Mattia and Grinvald, Margarita and Siegwart, Roland and Tombari, Federico},
  date = {2020},
  abstract = {Transferring the style from one image onto another is a popular and widely studied task in computer vision. Yet, style transfer in the 3D setting remains a largely unexplored problem. To our knowledge, we propose the first learning-based approach for style transfer between 3D objects based on disentangled content and style representations. The proposed method can synthesize new 3D shapes both in the form of point clouds and meshes, combining the content and style of a source and target 3D model to generate a novel shape that resembles in style the target while retaining the source content. Furthermore, we extend our technique to implicitly learn the multimodal style distribution of the chosen domains. By sampling style codes from the learned distributions, we increase the variety of styles that our model can confer to an input shape. Experimental results validate the effectiveness of the proposed 3D style transfer method on a number of benchmarks. The implementation of our framework will be released upon acceptance.}
}

@article{Seo.2017,
  title = {Avatar {{Face Recognition}} and {{Self-Presence}}},
  author = {Seo, Youngnam and Kim, Minkyung and Jung, Younbo and Lee, Doohwang},
  date = {2017},
  journaltitle = {Computers in Human Behavior},
  volume = {69},
  pages = {120--127},
  issn = {07475632},
  doi = {10.1016/j.chb.2016.12.020},
  abstract = {The present study investigated how users felt a sense of self-presence when they were exposed to self-relevant avatars. Specifically, this study used both self-report and electroencephalogram (EEG) measures to explore how users would psychologically and physiologically respond to avatars wearing their own faces. Twenty-five undergraduate students participated in an EEG-based experiment that employed a 2 × 4 within-subjects factorial design. In the experiment, each of the participants was randomly exposed to eight combinations of two types of image presentations (photo image vs. avatar image) and four categories of human faces (self-faces vs. famous faces vs. ideal faces vs. unfamiliar faces). The self-report result data indicated that participants felt significantly higher senses of physical similarity to and identification with the characters when they were exposed to their own faces in the form of a photo image. The findings of the EEG data also demonstrated that participants paid much more attention to their own faces than to any other faces regardless of the image presentation type.}
}

@article{Sharma.2005,
  title = {The {{CIEDE2000}} Color-Difference Formula: {{Implementation}} Notes, Supplementary Test Data, and Mathematical Observations},
  shorttitle = {The {{CIEDE2000}} Color-Difference Formula},
  author = {Sharma, Gaurav and Wu, Wencheng and Dalal, Edul N.},
  date = {2005-02},
  journaltitle = {Color Research \& Application},
  shortjournal = {Color Res. Appl.},
  volume = {30},
  number = {1},
  pages = {21--30},
  issn = {0361-2317, 1520-6378},
  doi = {10.1002/col.20070},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/col.20070},
  urldate = {2024-10-29},
  langid = {english},
  file = {/home/lucky/Zotero/storage/WALWEJBT/Sharma et al. - 2005 - The CIEDE2000 color-difference formula Implementation notes, supplementary test data, and mathemati.pdf}
}

@article{Shih.2014,
  title = {Style {{Transfer}} for {{Headshot Portraits}}},
  author = {Shih, YiChang and Paris, Sylvain and Barnes, Connelly and Freeman, William T. and Durand, Fredo},
  date = {2014},
  journaltitle = {07300301},
  issn = {07300301},
  doi = {10.1145/2601097.2601137},
  abstract = {Headshot portraits are a popular subject in photography but to achieve a compelling visual style requires advanced skills that a casual photographer will not have. Further, algorithms that automate or assist the stylization of generic photographs do not perform well on headshots due to the feature-specific, local retouching that a professional photographer typically applies to generate such portraits. We introduce a technique to transfer the style of an example headshot photo onto a new one. This can allow one to easily reproduce the look of renowned artists. At the core of our approach is a new multiscale technique to robustly transfer the local statistics of an example portrait onto a new one. This technique matches properties such as the local contrast and the overall lighting direction while being tolerant to the unavoidable differences between the faces of two different people. Additionally, because artists sometimes produce entire headshot collections in a common style, we show how to automatically find a good example to use as a reference for a given portrait, enabling style transfer without the user having to search for a suitable example for each input. We demonstrate our approach on data taken in a controlled environment as well as on a large set of photos downloaded from the Internet. We show that we can successfully handle styles by a variety of different artists.}
}

@book{Shirley.2002,
  title = {Fundamentals of {{Computer Graphics}}},
  author = {Shirley, Peter},
  date = {2002},
  publisher = {AK Peters},
  location = {Natick, Mass.},
  isbn = {1-56881-124-1}
}

@misc{StabilityAI.2023,
  title = {Learn {{How}} to {{Use}} the {{Stability Plugin}} to {{Generate Images}}, {{Animations}} and {{Textures Right}} inside {{Blender}}},
  author = {{Stability AI}},
  date = {2023}
}

@inproceedings{Tancik.2023,
  title = {Nerfstudio: {{A Modular Framework}} for {{Neural Radiance Field Development}}},
  shorttitle = {Nerfstudio},
  booktitle = {Special {{Interest Group}} on {{Computer Graphics}} and {{Interactive Techniques Conference Conference Proceedings}}},
  author = {Tancik, Matthew and Weber, Ethan and Ng, Evonne and Li, Ruilong and Yi, Brent and Kerr, Justin and Wang, Terrance and Kristoffersen, Alexander and Austin, Jake and Salahi, Kamyar and Ahuja, Abhik and McAllister, David and Kanazawa, Angjoo},
  date = {2023-07},
  eprint = {2302.04264},
  eprinttype = {arXiv},
  pages = {1--12},
  doi = {10.1145/3588432.3591516},
  abstract = {Neural Radiance Fields (NeRF) are a rapidly growing area of research with wide-ranging applications in computer vision, graphics, robotics, and more. In order to streamline the development and deployment of NeRF research, we propose a modular PyTorch framework, Nerfstudio. Our framework includes plug-and-play components for implementing NeRF-based methods, which make it easy for researchers and practitioners to incorporate NeRF into their projects. Additionally, the modular design enables support for extensive real-time visualization tools, streamlined pipelines for importing captured in-the-wild data, and tools for exporting to video, point cloud and mesh representations. The modularity of Nerfstudio enables the development of Nerfacto, our method that combines components from recent papers to achieve a balance between speed and quality, while also remaining flexible to future modifications. To promote community-driven development, all associated code and data are made publicly available with open-source licensing at https://nerf.studio.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics}
}

@article{Vachha.2024,
  title = {Instruct-{{GS2GS}}: {{Editing 3D Gaussian Splats}} with {{Instructions}}},
  author = {Vachha, Cyrus and Haque, Ayaan},
  date = {2024},
  keywords = {No DOI found}
}

@inproceedings{Varlamova.2022,
  title = {Survey of {{Approaches}} to {{Non-Realistic Neural Style Transfer}}},
  booktitle = {Proceedings of the 31st {{Conference}} of {{Open Innovations Association FRUCT}}},
  author = {Varlamova, Arina and Kitov, Victor},
  editor = {Balandin, Sergey I. and Shatalova, Tatiana},
  date = {2022},
  pages = {355--362},
  publisher = {IEEE},
  location = {Piscataway, NJ},
  doi = {10.23919/FRUCT54823.2022.9770893},
  isbn = {978-952-69244-7-2}
}

@misc{Waczynska.2024,
  title = {{{GaMeS}}: {{Mesh-Based Adapting}} and {{Modification}} of {{Gaussian Splatting}}},
  shorttitle = {{{GaMeS}}},
  author = {Waczyńska, Joanna and Borycki, Piotr and Tadeja, Sławomir and Tabor, Jacek and Spurek, Przemysław},
  date = {2024-02},
  eprint = {2402.01459},
  eprinttype = {arXiv},
  eprintclass = {cs},
  abstract = {Recently, a range of neural network-based methods for image rendering have been introduced. One such widely-researched neural radiance field (NeRF) relies on a neural network to represent 3D scenes, allowing for realistic view synthesis from a small number of 2D images. However, most NeRF models are constrained by long training and inference times. In comparison, Gaussian Splatting (GS) is a novel, state-of-the-art technique for rendering points in a 3D scene by approximating their contribution to image pixels through Gaussian distributions, warranting fast training and swift, real-time rendering. A drawback of GS is the absence of a well-defined approach for its conditioning due to the necessity to condition several hundred thousand Gaussian components. To solve this, we introduce the Gaussian Mesh Splatting (GaMeS) model, which allows modification of Gaussian components in a similar way as meshes. We parameterize each Gaussian component by the vertices of the mesh face. Furthermore, our model needs mesh initialization on input or estimated mesh during training. We also define Gaussian splats solely based on their location on the mesh, allowing for automatic adjustments in position, scale, and rotation during animation. As a result, we obtain a real-time rendering of editable GS.},
  langid = {english},
  organization = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{Wang.2004,
  title = {Image {{Quality Assessment}}: {{From Error Visibility}} to {{Structural Similarity}}},
  shorttitle = {Image {{Quality Assessment}}},
  author = {Wang, Z. and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  date = {2004-04},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {13},
  number = {4},
  pages = {600--612},
  issn = {1057-7149},
  doi = {10.1109/TIP.2003.819861},
  langid = {english}
}

@misc{Wang.2024,
  title = {{{GaussianEditor}}: {{Editing 3D Gaussians Delicately}} with {{Text Instructions}}},
  shorttitle = {{{GaussianEditor}}},
  author = {Wang, Junjie and Fang, Jiemin and Zhang, Xiaopeng and Xie, Lingxi and Tian, Qi},
  date = {2024-07},
  eprint = {2311.16037},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2311.16037},
  abstract = {Recently, impressive results have been achieved in 3D scene editing with text instructions based on a 2D diffusion model. However, current diffusion models primarily generate images by predicting noise in the latent space, and the editing is usually applied to the whole image, which makes it challenging to perform delicate, especially localized, editing for 3D scenes. Inspired by recent 3D Gaussian splatting, we propose a systematic framework, named GaussianEditor, to edit 3D scenes delicately via 3D Gaussians with text instructions. Benefiting from the explicit property of 3D Gaussians, we design a series of techniques to achieve delicate editing. Specifically, we first extract the region of interest (RoI) corresponding to the text instruction, aligning it to 3D Gaussians. The Gaussian RoI is further used to control the editing process. Our framework can achieve more delicate and precise editing of 3D scenes than previous methods while enjoying much faster training speed, i.e. within 20 minutes on a single V100 GPU, more than twice as fast as Instruct-NeRF2NeRF (45 minutes – 2 hours).},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics}
}

@misc{Wu.2022,
  title = {{{StyleAlign}}: {{Analysis}} and {{Applications}} of {{Aligned StyleGAN Models}}},
  shorttitle = {{{StyleAlign}}},
  author = {Wu, Zongze and Nitzan, Yotam and Shechtman, Eli and Lischinski, Dani},
  date = {2022-05},
  eprint = {2110.11323},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2110.11323},
  abstract = {In this paper, we perform an in-depth study of the properties and applications of aligned generative models. We refer to two models as aligned if they share the same architecture, and one of them (the child) is obtained from the other (the parent) via fine-tuning to another domain, a common practice in transfer learning. Several works already utilize some basic properties of aligned StyleGAN models to perform image-to-image translation. Here, we perform the first detailed exploration of model alignment, also focusing on StyleGAN. First, we empirically analyze aligned models and provide answers to important questions regarding their nature. In particular, we find that the child model's latent spaces are semantically aligned with those of the parent, inheriting incredibly rich semantics, even for distant data domains such as human faces and churches. Second, equipped with this better understanding, we leverage aligned models to solve a diverse set of tasks. In addition to image translation, we demonstrate fully automatic cross-domain image morphing. We further show that zero-shot vision tasks may be performed in the child domain, while relying exclusively on supervision in the parent domain. We demonstrate qualitatively and quantitatively that our approach yields state-of-the-art results, while requiring only simple fine-tuning and inversion.},
  organization = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning}
}

@misc{Wu.2024,
  title = {{{GaussCtrl}}: {{Multi-view Consistent Text-Driven 3D Gaussian Splatting Editing}}},
  author = {Wu, Jing and Bian, Jia-Wang and Li, Xinghui and Wang, Guangrun and Reid, Ian and Torr, Philip and Prisacariu, Victor Adrian},
  date = {2024},
  abstract = {We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed by the 3D Gaussian Splatting (3DGS). Our method first renders a collection of images by using the 3DGS and edits them by using a pre-trained 2D diffusion model (ControlNet) based on the input prompt, which is then used to optimise the 3D model. Our key contribution is multi-view consistent editing, which enables editing all images together instead of iteratively editing one image while updating the 3D model as in previous works. It leads to faster editing as well as higher visual quality. This is achieved by the two terms: (a) depth-conditioned editing that enforces geometric consistency across multi-view images by leveraging naturally consistent depth maps. (b) attention-based latent code alignment that unifies the appearance of edited images by conditioning their editing to several reference views through self and cross-view attention between images' latent representations. Experiments demonstrate that our method achieves faster editing and better visual results than previous state-of-the-art methods.}
}

@article{Xu.2021,
  title = {Voxel-{{Based Representation}} of {{3D Point Clouds}}: {{Methods}}, {{Applications}}, and {{Its Potential Use}} in the {{Construction Industry}}},
  author = {Xu, Yusheng and Tong, Xiaohua and Stilla, Uwe},
  date = {2021},
  journaltitle = {Automation in Construction},
  volume = {126},
  pages = {103675},
  issn = {0926-5805},
  doi = {10.1016/j.autcon.2021.103675},
  abstract = {Point clouds acquired through laser scanning and stereo vision techniques have been applied in a wide range of applications, proving to be optimal sources for mapping 3D urban scenes. Point clouds provide 3D spatial coordinates of geometric surfaces, describing the real 3D world with both geometric information and attributes. However, unlike 2D images, raw point clouds are usually unstructured and contain no semantic, geometric, or topological information of objects. This lack of an adequate data structure is a bottleneck for the pre-processing or further application of raw point clouds. Thus, it is generally necessary to organize and structure the 3D discrete points into a higher-level representation, such as voxels. Using voxels to represent discrete points is a common and effective way to organize and structure 3D point clouds. Voxels, similar to pixels in an image, are abstracted 3D units with pre-defined volumes, positions, and attributes, which can be used to structurally represent discrete points in a topologically explicit and information-rich manner. Although methods and algorithms for point clouds in various fields have been frequently reported throughout the last decade, there have been very few reviews summarizing and discussing the voxel-based representation of 3D point clouds in urban scenarios. Therefore, this paper aims to conduct a thorough review of the state-of-the-art methods and applications of voxel-based point cloud representations from a collection of papers in the recent decade. In particular, we focus on the creation and utilization of voxels, as well as the strengths and weaknesses of various methods using voxels. Moreover, we also provide an analysis of the potential of using voxel-based representations in the construction industry. Finally, we provide recommendations on future research directions regarding the future tendency of the voxel-based point cloud representations and its improvements.}
}

@misc{Yang.2023,
  title = {{{3DStyle-diffusion}}: {{Pursuing Fine-Grained Text-Driven 3D Stylization}} with {{2D Diffusion Models}}},
  author = {Yang, Haibo and Chen, Yang and Pan, Yingwei and Yao, Ting and Chen, Zhineng and Mei, Tao},
  date = {2023},
  abstract = {3D content creation via text-driven stylization has played a fundamental challenge to multimedia and graphics community. Recent advances of cross-modal foundation models (e.g., CLIP) have made this problem feasible. Those approaches commonly leverage CLIP to align the holistic semantics of stylized mesh with the given text prompt. Nevertheless, it is not trivial to enable more controllable stylization of fine-grained details in 3D meshes solely based on such semantic-level cross-modal supervision. In this work, we propose a new 3DStyle-Diffusion model that triggers fine-grained stylization of 3D meshes with additional controllable appearance and geometric guidance from 2D Diffusion models. Technically, 3DStyle-Diffusion first parameterizes the texture of 3D mesh into reflectance properties and scene lighting using implicit MLP networks. Meanwhile, an accurate depth map of each sampled view is achieved conditioned on 3D mesh. Then, 3DStyle-Diffusion leverages a pre-trained controllable 2D Diffusion model to guide the learning of rendered images, encouraging the synthesized image of each view semantically aligned with text prompt and geometrically consistent with depth map. This way elegantly integrates both image rendering via implicit MLP networks and diffusion process of image synthesis in an end-to-end fashion, enabling a high-quality fine-grained stylization of 3D meshes. We also build a new dataset derived from Objaverse and the evaluation protocol for this task. Through both qualitative and quantitative experiments, we validate the capability of our 3DStyle-Diffusion. Source code and data are available at {$<$}a href="https://github.com/yanghb22-fdu/3DStyle-Diffusion-Official"{$>$}https://github.com/yanghb22-fdu/3DStyle-Diffusion-Official{$<$}/a{$>$}.}
}

@misc{Ye.2023,
  title = {Mathematical {{Supplement}} for the \textbackslash textbackslash Texttt\{gsplat\}\textbackslash{} {{Library}}},
  author = {Ye, Vickie and Kanazawa, Angjoo},
  date = {2023-12},
  eprint = {2312.02121},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2312.02121},
  abstract = {This report provides the mathematical details of the gsplat library, a modular toolbox for efficient differentiable Gaussian splatting, as proposed by Kerbl et al. It provides a self-contained reference for the computations involved in the forward and backward passes of differentiable Gaussian splatting. To facilitate practical usage and development, we provide a user friendly Python API that exposes each component of the forward and backward passes in rasterization at github.com/nerfstudio-project/gsplat .},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Mathematical Software,Mathematics - Numerical Analysis}
}

@inproceedings{Yeh.2023,
  title = {Navigating {{Text-To-Image Customization}}: {{From LyCORIS Fine-Tuning}} to {{Model Evaluation}}},
  shorttitle = {Navigating {{Text-To-Image Customization}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Yeh, Shih-Ying and Hsieh, Yu-Guan and Gao, Zhidong and Yang, Bernard B. W. and Oh, Giyeong and Gong, Yanmin},
  date = {2023-10},
  abstract = {Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts. Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field. However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation. Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion. Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.}
}

@article{Yin.2021,
  title = {{{3DStyleNet}}: {{Creating 3D Shapes}} with {{Geometric}} and {{Texture Style Variations}}},
  author = {Yin, Kangxue and Gao, Jun and Shugrina, Maria and Khamis, Sameh and Fidler, Sanja},
  date = {2021},
  doi = {10.1109/ICCV48922.2021.01223},
  abstract = {We propose a method to create plausible geometric and texture style variations of 3D objects in the quest to democratize 3D content creation. Given a pair of textured source and target objects, our method predicts a part-aware affine transformation field that naturally warps the source shape to imitate the overall geometric style of the target. In addition, the texture style of the target is transferred to the warped source object with the help of a multi-view differentiable renderer. Our model, 3DStyleNet, is composed of two sub-networks trained in two stages. First, the geometric style network is trained on a large set of untextured 3D shapes. Second, we jointly optimize our geometric style network and a pre-trained image style transfer network with losses defined over both the geometry and the rendering of the result. Given a small set of high-quality textured objects, our method can create many novel stylized shapes, resulting in effortless 3D content creation and style-ware data augmentation. We showcase our approach qualitatively on 3D content stylization, and provide user studies to validate the quality of our results. In addition, our method can serve as a valuable tool to create 3D data augmentations for computer vision tasks. Extensive quantitative analysis shows that 3DStyleNet outperforms alternative data augmentation techniques for the downstream task of single-image 3D reconstruction.}
}

@misc{Zhang.2018,
  title = {The {{Unreasonable Effectiveness}} of {{Deep Features}} as a {{Perceptual Metric}}},
  author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
  date = {2018-04},
  eprint = {1801.03924},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1801.03924},
  abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
  organization = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics}
}

@online{Zhang.2018a,
  title = {The {{Unreasonable Effectiveness}} of {{Deep Features}} as a {{Perceptual Metric}}},
  author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
  date = {2018-04-10},
  eprint = {1801.03924},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1801.03924},
  url = {http://arxiv.org/abs/1801.03924},
  urldate = {2024-10-27},
  abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/home/lucky/Zotero/storage/ENVVZZZB/Zhang et al. - 2018 - The Unreasonable Effectiveness of Deep Features as a Perceptual Metric.pdf;/home/lucky/Zotero/storage/39F5XRT8/1801.html}
}

@article{Zhang.2023,
  title = {Adding {{Conditional Control}} to {{Text-to-Image Diffusion Models}}},
  author = {Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
  date = {2023},
  doi = {10.1109/ICCV51070.2023.00355},
  abstract = {We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with "zero convolutions" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, eg, edges, depth, segmentation, human pose, etc, with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small ({$<$}50k) and large ({$>$}1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.}
}

@misc{Zhang.2023a,
  title = {Text-{{Guided Generation}} and {{Editing}} of {{Compositional 3D Avatars}}},
  author = {Zhang, Hao and Feng, Yao and Kulits, Peter and Wen, Yandong and Thies, Justus and Black, Michael J.},
  date = {2023},
  abstract = {Our goal is to create a realistic 3D facial avatar with hair and accessories using only a text description. While this challenge has attracted significant recent interest, existing methods either lack realism, produce unrealistic shapes, or do not support editing, such as modifications to the hairstyle. We argue that existing methods are limited because they employ a monolithic modeling approach, using a single representation for the head, face, hair, and accessories. Our observation is that the hair and face, for example, have very different structural qualities that benefit from different representations. Building on this insight, we generate avatars with a compositional model, in which the head, face, and upper body are represented with traditional 3D meshes, and the hair, clothing, and accessories with neural radiance fields (NeRF). The model-based mesh representation provides a strong geometric prior for the face region, improving realism while enabling editing of the person's appearance. By using NeRFs to represent the remaining components, our method is able to model and synthesize parts with complex geometry and appearance, such as curly hair and fluffy scarves. Our novel system synthesizes these high-quality compositional avatars from text descriptions. The experimental results demonstrate that our method, Text-guided generation and Editing of Compositional Avatars (TECA), produces avatars that are more realistic than those of recent methods while being editable because of their compositional nature. For example, our TECA enables the seamless transfer of compositional features like hairstyles, scarves, and other accessories between avatars. This capability supports applications such as virtual try-on.}
}

@inproceedings{Zhang.2024,
  title = {{{CoARF}}: {{Controllable 3D Artistic Style Transfer}} for {{Radiance Fields}}},
  shorttitle = {{{CoARF}}},
  booktitle = {2024 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  author = {Zhang, Deheng and Fernandez-Labrador, Clara and Schroers, Christopher},
  date = {2024-03},
  pages = {612--622},
  publisher = {IEEE},
  location = {Davos, Switzerland},
  doi = {10.1109/3DV62453.2024.00022},
  abstract = {Creating artistic 3D scenes can be time-consuming and requires specialized knowledge. To address this, recent works such as ARF [57], use a radiance field-based approach with style constraints to generate 3D scenes that resemble a style image provided by the user. However, these methods lack fine-grained control over the resulting scenes. In this paper, we introduce Controllable Artistic Radiance Fields (CoARF), a novel algorithm for controllable 3D scene stylization. CoARF enables style transfer for specified objects, compositional 3D style transfer and semanticaware style transfer. We achieve controllability using segmentation masks with different label-dependent loss functions. We also propose a semantic-aware nearest neighbor matching algorithm to improve the style transfer quality. Our extensive experiments demonstrate that CoARF provides user-specified controllability of style transfer and superior style transfer quality with more precise feature matching.},
  isbn = {9798350362459}
}
